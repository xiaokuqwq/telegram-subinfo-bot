import asyncio
import base64
import re
import time
import html
import logging
import os
import sys
from datetime import datetime
from io import BytesIO

import aiohttp
import yaml
from telegram import Update, constants
from telegram.ext import ApplicationBuilder, MessageHandler, filters, ContextTypes
from telegram.request import HTTPXRequest
from dotenv import load_dotenv

# --- åŠ è½½ç¯å¢ƒå˜é‡ ---
load_dotenv()

# --- æ—¥å¿—é…ç½® ---
logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)

# --- é™æ€é…ç½® ---
TOKEN = os.getenv("TELEGRAM_BOT_TOKEN", "").strip()
PROXY_URL = os.getenv("TELEGRAM_PROXY_URL", "").strip()
REMOTE_MAPPINGS_URL = "https://raw.githubusercontent.com/Hyy800/Quantumult-X/refs/heads/Nana/ymys.txt"
REMOTE_CONFIG_MAPPINGS = {}

# åœ°åŒºè¯†åˆ«è§„åˆ™ (åŸç‰ˆ)
REGION_RULES = [
    ('é¦™æ¸¯', ['é¦™æ¸¯', 'hong kong', 'hongkong', 'hk', 'hkg']),
    ('å°æ¹¾', ['å°æ¹¾', 'taiwan', 'tw', 'taipei', 'tpe']),
    ('æ—¥æœ¬', ['æ—¥æœ¬', 'japan', 'jp', 'tokyo', 'osaka', 'jap']),
    ('æ–°åŠ å¡', ['æ–°åŠ å¡', 'singapore', 'sg', 'sgp']),
    ('éŸ©å›½', ['éŸ©å›½', 'korea', 'kr', 'seoul', 'kor']),
    ('ç¾å›½', ['ç¾å›½', 'united states', 'us', 'usa', 'los angeles', 'san jose']),
]

# å…¨å±€å˜é‡
GLOBAL_SEMAPHORE = asyncio.Semaphore(50)  # aiohttp æ€§èƒ½æ›´å¥½ï¼Œå¹¶å‘å¯ä»¥å¼€å¤§ä¸€ç‚¹
shared_session = None

# --- å·¥å…·å‡½æ•° ---

def format_size(size: float) -> str:
    units = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']
    level = 0
    while size >= 1024 and level < len(units) - 1:
        size /= 1024
        level += 1
    return f"{size:.2f} {units[level]}"

def parse_user_info(header: str):
    info = {}
    for part in header.split(';'):
        if '=' in part:
            k, v = part.split('=', 1)
            info[k.strip().lower()] = v.strip()
    return info

def analyze_regions(proxies):
    stats = {}
    for p in proxies:
        name = str(p.get('name', '')).lower()
        found = False
        for region, keywords in REGION_RULES:
            if any(k in name for k in keywords):
                stats[region] = stats.get(region, 0) + 1
                found = True
                break
        if not found:
            stats['å…¶ä»–'] = stats.get('å…¶ä»–', 0) + 1
    
    if not stats: return "æ— æœ‰æ•ˆèŠ‚ç‚¹"
    return " | ".join([f"{k}:{v}" for k, v in stats.items()])

async def fetch_node_info(url: str):
    """ä½¿ç”¨ aiohttp è·å–èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯"""
    try:
        async with shared_session.get(url, timeout=10) as resp:
            data = await resp.text()
            if 'proxies' in data:
                config = yaml.safe_load(data)
                proxies = config.get('proxies', [])
                return {"count": len(proxies), "detail": analyze_regions(proxies)}
            try:
                missing_padding = len(data) % 4
                if missing_padding: data += '=' * (4 - missing_padding)
                decoded = base64.b64decode(data).decode('utf-8')
                lines = [l for l in decoded.splitlines() if '://' in l]
                if lines: return {"count": len(lines), "detail": f"{len(lines)}ä¸ªé€šç”¨èŠ‚ç‚¹"}
            except Exception as e:
                logger.debug(f"Failed to parse node info for {url}: {e}")
    except Exception as e:
        logger.debug(f"Failed to fetch node info for {url}: {e}")
    return None

async def process_sub(url: str):
    """aiohttp æ ¸å¿ƒå¤„ç†é€»è¾‘"""
    async with GLOBAL_SEMAPHORE:
        try:
            headers = {'User-Agent': 'Clash-Verge/1.0.0'}
            async with shared_session.get(url, headers=headers, timeout=15) as resp:
                if resp.status != 200:
                    return {"success": False, "url": url, "error": f"HTTP {resp.status}"}
                
                user_info = resp.headers.get('subscription-userinfo')
                if not user_info:
                    return {"success": False, "url": url, "error": "ä¸è¿”å›æµé‡Header"}
                
                info = parse_user_info(user_info)
                u, d, t, e = int(info.get('upload', 0)), int(info.get('download', 0)), int(info.get('total', 0)), int(info.get('expire', 0))
                
                used = u + d
                percent = round((used / t) * 100, 2) if t > 0 else 0
                name = next((v for k, v in REMOTE_CONFIG_MAPPINGS.items() if k in url), "æœªçŸ¥æœºåœº")
                node = await fetch_node_info(url)
                
                return {
                    "success": True, "url": url, "name": name, "total": t, "used": used,
                    "remain": max(0, t - used), "percent": percent, "expire_ts": e,
                    "node": node, "up": u, "down": d
                }
        except Exception as err:
            logger.error(f"Error processing {url}: {err}")
            return {"success": False, "url": url, "error": "è¿æ¥è¶…æ—¶/å¼‚å¸¸"}

# --- æ¶ˆæ¯å¤„ç†å™¨ ---

async def handle_request(update: Update, context: ContextTypes.DEFAULT_TYPE):
    msg = update.effective_message
    if not msg: return

    content = msg.text or msg.caption or ""
    urls = re.findall(r'https?://[^\s]+', content)

    if msg.document and (msg.document.file_name.endswith('.txt') or msg.document.mime_type == 'text/plain'):
        file = await msg.document.get_file()
        byte_content = await file.download_as_bytearray()
        urls.extend(re.findall(r'https?://[^\s]+', byte_content.decode('utf-8', errors='ignore')))

    urls = list(dict.fromkeys(urls))
    if not urls: return

    status_msg = await msg.reply_text("ğŸš€ aiohttp æé€Ÿå¼•æ“å¤„ç†ä¸­...")

    tasks = [process_sub(url) for url in urls]
    responses = await asyncio.gather(*tasks)

    results = []
    for res in responses:
        safe_url = html.escape(res['url'])
        if not res["success"]:
            results.append(f"âŒ <b>è§£æå¤±è´¥</b>\nè®¢é˜…: <code>{safe_url}</code>\nåŸå› : {res['error']}")
            continue
        
        filled = min(15, int(res['percent'] / 6.6))
        bar = "â–ˆ" * filled + "â–‘" * (15 - filled)
        expire_date = datetime.fromtimestamp(res['expire_ts']).strftime('%Y-%m-%d') if res['expire_ts'] > 0 else "æ°¸ä¹…/æœªçŸ¥"
        
        output = (
            f"ğŸ“„ <b>æœºåœºåç§°</b>: <code>{html.escape(res['name'])}</code>\n"
            f"ğŸ·ï¸ <b>è®¢é˜…é“¾æ¥</b>: <code>{safe_url}</code>\n"
            f"ğŸ“Š <b>æµé‡ä¿¡æ¯</b>:\n"
            f"é¢„è§ˆ: <code>[{bar}] {res['percent']}%</code>\n"
            f"æ€»æµé‡: <code>{format_size(res['total'])}</code>\n"
            f"å·²ä½¿ç”¨: <code>{format_size(res['used'])}</code> (â†‘{format_size(res['up'])} â†“{format_size(res['down'])})\n"
            f"å‰©ä½™é‡: <code>{format_size(res['remain'])}</code>\n"
            f"â° <b>åˆ°æœŸæ—¶é—´</b>: <code>{expire_date}</code>\n"
        )
        if res['node']:
            output += f"ğŸŒ <b>èŠ‚ç‚¹ä¿¡æ¯</b>: <code>{res['node']['count']}ä¸ªèŠ‚ç‚¹ ({res['node']['detail']})</code>"
        
        results.append(output)

    final_text = "\n" + ("="*20) + "\n\n".join(results)

    if len(final_text) > 4000:
        clean_text = re.sub('<[^<]+?>', '', final_text)
        bio = BytesIO(clean_text.encode())
        bio.name = "aio_report.txt"
        await msg.reply_document(document=bio, caption="âœ… æ‰¹é‡æŸ¥è¯¢å®Œæˆ")
        await status_msg.delete()
    else:
        await status_msg.edit_text(final_text, parse_mode=constants.ParseMode.HTML, disable_web_page_preview=True)

# --- å…¥å£ ---

async def main():
    global shared_session

    # 1. Token æ ¡éªŒ
    if not TOKEN:
        logger.error("âŒ æœªè®¾ç½® Bot Tokenï¼è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® TELEGRAM_BOT_TOKENã€‚")
        sys.exit(1)
    
    # ç®€å•æ­£åˆ™æ ¡éªŒ (æ•°å­—:å­—ç¬¦)
    if not re.match(r'^\d+:[A-Za-z0-9_-]+$', TOKEN):
        logger.error(f"âŒ Bot Token æ ¼å¼é”™è¯¯: '{TOKEN}'ã€‚è¯·æ£€æŸ¥ .env æ–‡ä»¶ã€‚")
        sys.exit(1)

    # 2. åˆå§‹åŒ– aiohttp è¿æ¥æ± 
    connector = aiohttp.TCPConnector(limit=100, ttl_dns_cache=300)
    shared_session = aiohttp.ClientSession(connector=connector)

    try:
        # 3. åŠ è½½æ˜ å°„
        try:
            async with shared_session.get(REMOTE_MAPPINGS_URL) as r:
                text = await r.text()
                for line in text.splitlines():
                    if '=' in line and not line.startswith('#'):
                        k, v = line.split('=', 1)
                        REMOTE_CONFIG_MAPPINGS[k.strip()] = v.strip()
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½è¿œç¨‹æ˜ å°„å¤±è´¥: {e}")

        # 4. é…ç½® Telegram Bot (æ”¯æŒä»£ç†)
        request_kwargs = {}
        if PROXY_URL:
            logger.info(f"ğŸŒ ä½¿ç”¨ä»£ç†: {PROXY_URL}")
            request_kwargs["proxy"] = PROXY_URL
        
        req = HTTPXRequest(connection_pool_size=100, **request_kwargs)

        app = ApplicationBuilder().token(TOKEN).request(req).concurrent_updates(True).build()
        app.add_handler(MessageHandler(filters.TEXT | filters.Document.Category("text/plain"), handle_request))
        
        print(f">>> ğŸ¤– Bot å¯åŠ¨ä¸­... (Token: {TOKEN[:5]}...)")
        print(">>> aiohttp æé€Ÿå¹¶å‘ç‰ˆå¯åŠ¨...")
        
        async with app:
            await app.initialize()
            await app.start()
            await app.updater.start_polling()
            await asyncio.Event().wait()
            
    except Exception as e:
        logger.error(f"âŒ è¿è¡Œæ—¶é”™è¯¯: {e}")
        raise
    finally:
        if shared_session:
            await shared_session.close()
            logger.info("ğŸ›‘ aiohttp è¿æ¥æ± å·²å…³é—­")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        pass
